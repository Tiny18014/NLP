import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

for res in ['punkt', 'stopwords', 'wordnet', 'punkt_tab']:
    nltk.download(res)

text1 = "Mumbai is the financial capital of India. It is known for Bollywood and its street food. I love going there"
text2 = "The Taj Mahal is located in Agra. It was built by Emperor Shah Jahan in memory of his wife."

sentences1 = sent_tokenize(text1)
sentences2 = sent_tokenize(text2)
print(sentences1)


words1 = word_tokenize(text1)
words2 = word_tokenize(text2)
print(words1)

stop_words = set(stopwords.words('english'))
filtered_words1 = [word for word in words1 if word.lower() not in stop_words]
print(filtered_words1)

stemmer = PorterStemmer()
stemmed_words1 = [stemmer.stem(word) for word in filtered_words1]
print(stemmed_words1)

lemmatizer = WordNetLemmatizer()
lemmatized_words1 = [lemmatizer.lemmatize(word) for word in filtered_words1]
print(lemmatized_words1)
